{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "diBj19cr3h-T"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QR0eiAv-Tz4-"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEU_-sHg7Jij"
      },
      "outputs": [],
      "source": [
        "!pip install pyvi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_eTK0uG0GZw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import RobertaForSequenceClassification, AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJYTk1Wc7CFE"
      },
      "outputs": [],
      "source": [
        "import unicodedata\n",
        "import regex as re\n",
        "from pyvi import ViTokenizer\n",
        "\n",
        "bang_nguyen_am = [['a', 'à', 'á', 'ả', 'ã', 'ạ', 'a'],\n",
        "                  ['ă', 'ằ', 'ắ', 'ẳ', 'ẵ', 'ặ', 'aw'],\n",
        "                  ['â', 'ầ', 'ấ', 'ẩ', 'ẫ', 'ậ', 'aa'],\n",
        "                  ['e', 'è', 'é', 'ẻ', 'ẽ', 'ẹ', 'e'],\n",
        "                  ['ê', 'ề', 'ế', 'ể', 'ễ', 'ệ', 'ee'],\n",
        "                  ['i', 'ì', 'í', 'ỉ', 'ĩ', 'ị', 'i'],\n",
        "                  ['o', 'ò', 'ó', 'ỏ', 'õ', 'ọ', 'o'],\n",
        "                  ['ô', 'ồ', 'ố', 'ổ', 'ỗ', 'ộ', 'oo'],\n",
        "                  ['ơ', 'ờ', 'ớ', 'ở', 'ỡ', 'ợ', 'ow'],\n",
        "                  ['u', 'ù', 'ú', 'ủ', 'ũ', 'ụ', 'u'],\n",
        "                  ['ư', 'ừ', 'ứ', 'ử', 'ữ', 'ự', 'uw'],\n",
        "                  ['y', 'ỳ', 'ý', 'ỷ', 'ỹ', 'ỵ', 'y']]\n",
        "bang_ky_tu_dau = ['', 'f', 's', 'r', 'x', 'j']\n",
        "\n",
        "vowel_to_ids = {}\n",
        "for i in range(len(bang_nguyen_am)):\n",
        "    for j in range(len(bang_nguyen_am[i]) - 1):\n",
        "        vowel_to_ids[bang_nguyen_am[i][j]] = (i, j)\n",
        "\n",
        "def is_valid_vietnam_word(word):\n",
        "    chars = word.split()\n",
        "    vowel_index = -1\n",
        "    for index, char in enumerate(chars):\n",
        "        x, y = vowel_to_ids.get(char, (-1, -1))\n",
        "        if x != -1:\n",
        "            if vowel_index == -1:\n",
        "                vowel_index = index\n",
        "            else:\n",
        "                if index - vowel_index != 1:\n",
        "                    return False\n",
        "                vowel_index = index\n",
        "    return True\n",
        "\n",
        "# lowercase sentences\n",
        "def to_lowercase(text):\n",
        "    return text.lower()\n",
        "\n",
        "# delete links\n",
        "# normalize unicode\n",
        "def normalize_unicode(text):\n",
        "    text = unicodedata.normalize('NFC', text)\n",
        "    return text\n",
        "\n",
        "# delete redundant characters\n",
        "# delete redundant spaces\n",
        "def remove_redundant_characters(text):\n",
        "    text = re.sub(r'[^\\s\\wáàảãạăắằẳẵặâấầẩẫậéèẻẽẹêếềểễệóòỏõọôốồổỗộơớờởỡợíìỉĩịúùủũụưứừửữựýỳỷỹỵđ_]', ' ', text )\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "# normalize accented letters for each word\n",
        "def normalize_accented_letters_for_each_word(word):\n",
        "    if not is_valid_vietnam_word(word):\n",
        "        return word\n",
        "\n",
        "    chars = word.split()\n",
        "    dau_cau = 0\n",
        "    index_of_vowel = []\n",
        "    qu_or_gi =False\n",
        "\n",
        "    for index, char in enumerate(chars):\n",
        "        x, y = vowel_to_ids.get(char, (-1, -1))\n",
        "        if x == -1:\n",
        "            continue\n",
        "        elif x == 9: #check qu\n",
        "            if index != 0 and chars[index -1] == 'q':\n",
        "                chars[index] = 'u'\n",
        "                qu_or_gi = True\n",
        "        elif x == 5: #check gi\n",
        "            if index != 0 and chars[index - 1] == 'g':\n",
        "                chars[index] = 'i'\n",
        "                qu_or_gi = True\n",
        "        if y != 0:\n",
        "            dau_cau = y\n",
        "            chars[index] = bang_nguyen_am[x][0]\n",
        "        if not qu_or_gi or index != 1:\n",
        "            index_of_vowel.append(index)\n",
        "    if len(index_of_vowel) < 2:\n",
        "        if qu_or_gi:\n",
        "            if len(chars) == 2:\n",
        "                x, y = vowel_to_ids.get(chars[1])\n",
        "                chars[1] = bang_nguyen_am[x][dau_cau]\n",
        "            else:\n",
        "                x, y = vowel_to_ids.get(chars[2], (-1, -1))\n",
        "                if x != -1:\n",
        "                    chars[2] = bang_nguyen_am[x][dau_cau]\n",
        "                else:\n",
        "                    chars[1] = bang_nguyen_am[5][dau_cau] if chars[1] == 'i' else bang_nguyen_am[9][dau_cau]\n",
        "            return word\n",
        "        return word\n",
        "\n",
        "    for index in index_of_vowel:\n",
        "        x, y - vowel_to_ids[chars[index]]\n",
        "        if x == 4 or x == 8: # ê, ơ\n",
        "            chars[index] = bang_nguyen_am[x][dau_cau]\n",
        "            return ''.join(chars)\n",
        "\n",
        "    if len(index_of_vowel) == 2:\n",
        "        if index_of_vowel[-1] == len(chars) - 1:\n",
        "            x, y = vowel_to_ids[chars[index_of_vowel[0]]]\n",
        "            chars[index_of_vowel[0]] = bang_nguyen_am[x][dau_cau]\n",
        "        else:\n",
        "            x, y = vowel_to_ids[chars[index_of_vowel[1]]]\n",
        "            chars[index_of_vowel[1]] = bang_nguyen_am[x][dau_cau]\n",
        "    else:\n",
        "        x, y = vowel_to_ids[chars[index_of_vowel[1]]]\n",
        "        chars[index_of_vowel[1]] = bang_nguyen_am[x][dau_cau]\n",
        "    return ''.join(chars)\n",
        "\n",
        "# normalize accented letters for each sentence\n",
        "def normalize_accented_letters_for_each_senence(sentence):\n",
        "    sentence = sentence.lower()\n",
        "    words = sentence.split()\n",
        "    for index, word in enumerate(words):\n",
        "        cw = re.sub(r'(^\\p{P}*)([p{L}.]*\\p{L}+)(\\p{P}*$)', r'\\1/\\2/\\3', word).split('/')\n",
        "        if len(cw) == 3:\n",
        "            cw[1] = normalize_accented_letters_for_each_word(cw[1])\n",
        "        words[index] = ''.join(cw)\n",
        "    return ' '.join(words)\n",
        "\n",
        "# word tokenize\n",
        "def word_tokenization(text):\n",
        "    text = ViTokenizer.tokenize(text)\n",
        "    return text\n",
        "\n",
        "# delete teencode\n",
        "# remove stopwords\n",
        "with open('stopword.txt', 'r', encoding = 'utf-8') as f:\n",
        "    stopword_list = f.read().split('\\n')\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    text = text.split()\n",
        "    non_sw_text = []\n",
        "    for word in text:\n",
        "       if word not in stopword_list:\n",
        "           non_sw_text.append(word)\n",
        "    result = ' '.join([str(item) for item in non_sw_text])\n",
        "    return result\n",
        "\n",
        "def prepare_text(text):\n",
        "    text = to_lowercase(text)\n",
        "    text = normalize_unicode(text)\n",
        "    text = remove_redundant_characters(text)\n",
        "    text = normalize_accented_letters_for_each_senence(text)\n",
        "    text = word_tokenization(text)\n",
        "    text = remove_stopwords(text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2DPNn3e3qs_"
      },
      "outputs": [],
      "source": [
        "model = RobertaForSequenceClassification.from_pretrained(\"wonrax/phobert-base-vietnamese-sentiment\").to('cuda')\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"wonrax/phobert-base-vietnamese-sentiment\", use_fast=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUo4qVff4EK9"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('/content/news_text_28_5.csv')\n",
        "data[\"Negs\"] = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4FkKw8R4DUE"
      },
      "outputs": [],
      "source": [
        "def sentiment(sentence):\n",
        "  input_ids = torch.tensor([tokenizer.encode(sentence)]).to('cuda')\n",
        "  with torch.no_grad():\n",
        "      out = model(input_ids)\n",
        "      out = out.logits.softmax(dim=-1).tolist()\n",
        "      out = out[0]\n",
        "      if max(out)==out[0]:\n",
        "        return max(out)\n",
        "  return 0\n",
        "      # Output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4qR4TXcSY81"
      },
      "outputs": [],
      "source": [
        "def extract_value(text):\n",
        "  res = []\n",
        "  paragraphs = text.split(\"\\n\")\n",
        "  for paragraph in paragraphs:\n",
        "    sentences = paragraph.split(\". \")\n",
        "    if len(sentences) >=3:\n",
        "      res.append(prepare_text(sentences[0]))\n",
        "      res.append(prepare_text(sentences[len(sentences)-1]))\n",
        "    elif len(sentences) == 2:\n",
        "      res.append(prepare_text(sentences[0]))\n",
        "      res.append(prepare_text(sentences[1]))\n",
        "    else:\n",
        "      res.append(prepare_text(sentences[0]))\n",
        "  return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJ1rlPjKzMI0"
      },
      "outputs": [],
      "source": [
        "index = 0\n",
        "for idx, row in data.iterrows():\n",
        "  try:\n",
        "    temp_data = extract_value(row.text)\n",
        "    temp_data.append(prepare_text(row.title))\n",
        "  except:\n",
        "    temp_data = []\n",
        "  if len(temp_data)==0:\n",
        "    continue\n",
        "  else:\n",
        "    neg = []\n",
        "    for sentence in temp_data:\n",
        "      if sentence != \"\":\n",
        "        try:\n",
        "          neg.append(sentiment(sentence))\n",
        "        except:\n",
        "          neg.append(0)\n",
        "      else:\n",
        "        neg.append(0)\n",
        "  sum = 0\n",
        "  for ne in neg:\n",
        "    sum = sum+ne\n",
        "  row.Negs = sum/(len(temp_data))\n",
        "  index = index+1\n",
        "  print(index)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0Os9BKGar4C"
      },
      "outputs": [],
      "source": [
        "data.to_csv(\"Result_28_5.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_woznzJPqkB"
      },
      "outputs": [],
      "source": [
        "!cp \"/content/Result_28_5.csv\" \"/content/drive/MyDrive\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}